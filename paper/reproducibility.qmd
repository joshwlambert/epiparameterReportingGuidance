Reporting every aspect of a parameter estimate can be challenging, if not impossible, due to the myriad of methodological features, estimation outputs and metadata. Therefore, even if reported *unambiguously*, there is still the almost inevitable risk of *information loss* for all but the most simple estimation methods and results. It is thus imperative that readers of reported epidemiological parameter estimates are able to reproduce and verify values themselves.

Reproducibility of analysis relies on sharing code. We recommend a code sharing platform like GitHub, GitLab or BitBucket, but any platform that allows everyone to read and download the code will suffice, as long as clearly linked to where parameters are reported. Code and analyses are often updated iteratively, using a version control system (e.g. git)  and releasing versions ensures the version of the code run corresponds to the estimates being reported, and allows each version to be tagged with a unique identifier (e.g. DOI) (Ram, 2013). Platforms like Zenodo, Figshare and Dryad offer hosting of these versioned snapshots. For analyses that take a substantial amount of computational resources, it can be useful to also save a copy of the analysis output, to spare the reader having to rerun the code. 

In most cases the analysis will require also sharing the data along with the code. We recommend that when the data does not contain any confidential information, that it be made open and accessible, ideally with changed tracking using a version control system (e.g. git) and shared in the same place as the code (e.g. GitHub) (Ram, 2013). Anonymisation techniques allow data to be modified before sharing to remove personal identifiable information, which often will not influence the parameter estimation method. In cases where the original data cannot be shared then we recommend sharing a mock or synthetic dataset that has been generated to have the same epidemiological characteristics, and will allow other to rerun the analysis and arrive at a similar epidemiological estimate, again benefitting from having full knowledge of the methodological setup. There are several tools available to simulate synthetic epidemiological datasets ({simulist}, {serosim}, {epidemics}, among others). Sharing spreadsheets enables verification of formulas and identification of common issues (Powell et al. 2008).

The full transparency of code and data (or mock data) sharing allows the full methodology, parameterisation, estimates, including uncertainty, covariance, posteriors, etc., and metadata to be understood and utilised in downstream analyses. Redressing any *ambiguity* or *information loss* from the published estimates. It also allows methodological adjustments that may not have implemented for the reported estimate to be added, such as for real-time outbreak data, or for newly developed methods to be applied to existing data. Overall,  epidemiological parameters reported in the literature provide a surface-level result, but the full information and provenance of the values is in the code and data.
